# Week 4 - Linear Models, Regularization

## Basics
- What is linear regression?
- What is the linear regression function?
- How could we summarize the relationship between x and y?
- What is the simple linear regression model?
- What are the parameters/coefficients of this model?
- What is the fitted line?
- What  are the fitted values?
- What is the residual sum of squares?
- What is the intercepted slope?
- What is the standard error?
- What is the least square estimator, and what are their assumptions?
- How can we use hypothesis testing in linear regression? (Hint: look for t-statistic)]

## Multiple Regression

- What is the difference between multiple regression and simple linear regression?
- How is the least square estimation function changed? How does it relate to variance-covariance? 
- Why do we use the F-statistic in this context?

## Model selection.

 Define:

- Residual Standard error
- Mallows Cp statistic
- Akaike information criterion
- Bayesian information criterion
- Adjusted RÂ² 

- **How do we use these metrics to select for the best model?**

Extensions of linear models:

## What is subset selection? What is the algorithm for it?

- What is step-wise selection? What is the difference between forward and backwards selection? What are their algorithms?
- Regularization/Shrinkage methods
- What is the difference between shrinkage methods and selection methods?
- What is Ridge Regression? What is it trying to minimize? What are tuning parameter? What is shrinkage penalty?
- What is lasso Regression? What is it trying to minimize? What is the lasso penalty? What is the variable selection property of the Lasso?
- What are the differences between Lasso and Ridge Regression?

Optional: What is the relationship between regression and likelihood function? What is the bayesian interpretation of regularization?